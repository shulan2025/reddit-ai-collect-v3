name: Daily Reddit AI Collect v3.0

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©UTC 2:00 (åŒ—äº¬æ—¶é—´10:00)
  workflow_dispatch:
    inputs:
      target_count:
        description: 'é‡‡é›†ç›®æ ‡æ•°é‡'
        required: false
        default: '1000'
        type: string

jobs:
  incremental-crawl:
    runs-on: ubuntu-latest
    name: Reddit AIæ•°æ®å¢é‡é‡‡é›†
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run incremental crawl
        run: |
          echo "ğŸš€ å¼€å§‹å¢é‡é‡‡é›† - åªè¿‡æ»¤å½“æ—¥å·²é‡‡é›†æ•°æ®..."
          node scripts/incremental-crawl.js
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: 'reddit-ai-collect_v3/3.0.0 (by /u/ai_researcher)'
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Database Statistics
        if: success()
        run: |
          echo "ğŸ“Š è·å–æ•°æ®åº“ç»Ÿè®¡..."
          node -e "
          const CLOUDFLARE_API_TOKEN = process.env.CLOUDFLARE_API_TOKEN;
          const ACCOUNT_ID = process.env.ACCOUNT_ID;
          const DATABASE_ID = process.env.DATABASE_ID;
          
          async function getStats() {
            try {
              const response = await fetch(\`https://api.cloudflare.com/client/v4/accounts/\${ACCOUNT_ID}/d1/database/\${DATABASE_ID}/query\`, {
                method: 'POST',
                headers: {
                  'Authorization': \`Bearer \${CLOUDFLARE_API_TOKEN}\`,
                  'Content-Type': 'application/json'
                },
                body: JSON.stringify({ 
                  sql: 'SELECT collection_date, COUNT(*) as count FROM redditV2_posts GROUP BY collection_date ORDER BY collection_date DESC LIMIT 3;' 
                })
              });
              
              if (!response.ok) {
                throw new Error(\`HTTP \${response.status}\`);
              }
              
              const result = await response.json();
              const stats = result.result[0]?.results || [];
              
              console.log('ğŸ“Š æœ€è¿‘3æ—¥æ•°æ®ç»Ÿè®¡:');
              stats.forEach(row => {
                console.log(\`ğŸ“… \${row.collection_date}: \${row.count} æ¡å¸–å­\`);
              });
              
              const todayCount = stats.find(s => s.collection_date === new Date().toISOString().split('T')[0])?.count || 0;
              console.log(\`::notice title=ä»Šæ—¥é‡‡é›†å®Œæˆ::æ–°å¢ \${todayCount} æ¡å¸–å­\`);
              
            } catch (error) {
              console.error('âŒ ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥:', error.message);
            }
          }
          
          getStats();
          "
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: crawl-logs-${{ github.run_number }}
          path: data/
          retention-days: 3
