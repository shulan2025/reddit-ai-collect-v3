name: Daily Reddit AI Collect v3.0 Incremental Crawl

on:
  schedule:
    # 每日北京时间上午10:00执行 (UTC 02:00)
    - cron: '0 2 * * *'
  workflow_dispatch: # 允许手动触发
    inputs:
      crawl_type:
        description: '采集类型'
        required: true
        default: 'incremental'
        type: choice
        options:
        - incremental
        - full
      target_count:
        description: '目标采集数量'
        required: false
        default: '1000'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    name: Reddit AI数据采集
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Create data directory
        run: mkdir -p data

      - name: Run Incremental Crawl (Default)
        if: ${{ github.event.inputs.crawl_type == 'incremental' || github.event_name == 'schedule' }}
        run: |
          echo "🚀 开始增量采集 - 过滤今日已采集数据..."
          node scripts/incremental-crawl.js
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: 'reddit-ai-collect_v2/2.0.0 (by /u/ai_researcher)'
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Run Full Crawl
        if: ${{ github.event.inputs.crawl_type == 'full' }}
        run: |
          echo "🚀 开始完整采集 - 目标 ${{ github.event.inputs.target_count || '2000' }} 条帖子..."
          node scripts/full-crawl-2000.js
          echo "📥 开始数据库插入..."
          node scripts/direct-d1-insert.js
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: 'reddit-ai-collect_v2/2.0.0 (by /u/ai_researcher)'
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Upload Crawl Data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: crawl-data-${{ github.run_number }}-${{ github.event.inputs.crawl_type || 'incremental' }}
          path: data/
          retention-days: 7

      - name: Database Statistics
        if: success()
        run: |
          echo "📊 获取数据库统计信息..."
          node -e "
          const CLOUDFLARE_API_TOKEN = process.env.CLOUDFLARE_API_TOKEN;
          const ACCOUNT_ID = process.env.ACCOUNT_ID;
          const DATABASE_ID = process.env.DATABASE_ID;
          
          async function getStats() {
            const response = await fetch(\`https://api.cloudflare.com/client/v4/accounts/\${ACCOUNT_ID}/d1/database/\${DATABASE_ID}/query\`, {
              method: 'POST',
              headers: {
                'Authorization': \`Bearer \${CLOUDFLARE_API_TOKEN}\`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({ 
                sql: 'SELECT COUNT(*) as total, COUNT(DISTINCT subreddit) as subreddits, COUNT(DISTINCT collection_date) as days FROM redditV2_posts;' 
              })
            });
            
            const result = await response.json();
            const stats = result.result[0]?.results?.[0];
            
            console.log('📊 数据库统计:');
            console.log(\`📄 总帖子数: \${stats.total}\`);
            console.log(\`🏠 覆盖社区: \${stats.subreddits}\`);
            console.log(\`📅 采集天数: \${stats.days}\`);
            
            // 设置GitHub Actions输出
            console.log(\`::notice title=采集完成::总计\${stats.total}条帖子，覆盖\${stats.subreddits}个社区\`);
          }
          
          getStats().catch(console.error);
          "
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

  notify:
    runs-on: ubuntu-latest
    name: 通知状态
    needs: crawl
    if: always()
    steps:
      - name: Notify Success
        if: needs.crawl.result == 'success'
        run: |
          echo "✅ Reddit AI数据采集任务执行成功!"
          echo "🎯 采集类型: ${{ github.event.inputs.crawl_type || 'incremental' }}"
          echo "📅 执行时间: $(date)"
          
      - name: Notify Failure
        if: needs.crawl.result == 'failure'
        run: |
          echo "❌ Reddit AI数据采集任务执行失败!"
          echo "🎯 采集类型: ${{ github.event.inputs.crawl_type || 'incremental' }}"
          echo "📅 执行时间: $(date)"
          echo "::error title=采集失败::请检查日志并重新运行"
