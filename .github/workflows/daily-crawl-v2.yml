name: Daily Reddit AI Collect v3.0 Incremental Crawl

on:
  schedule:
    # æ¯æ—¥åŒ—äº¬æ—¶é—´ä¸Šåˆ10:00æ‰§è¡Œ (UTC 02:00)
    - cron: '0 2 * * *'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘
    inputs:
      crawl_type:
        description: 'é‡‡é›†ç±»å‹'
        required: true
        default: 'incremental'
        type: choice
        options:
        - incremental
        - full
      target_count:
        description: 'ç›®æ ‡é‡‡é›†æ•°é‡'
        required: false
        default: '1000'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    name: Reddit AIæ•°æ®é‡‡é›†
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Create data directory
        run: mkdir -p data

      - name: Run Incremental Crawl (Default)
        if: ${{ github.event.inputs.crawl_type == 'incremental' || github.event_name == 'schedule' }}
        run: |
          echo "ğŸš€ å¼€å§‹å¢é‡é‡‡é›† - è¿‡æ»¤ä»Šæ—¥å·²é‡‡é›†æ•°æ®..."
          node scripts/incremental-crawl.js
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: 'reddit-ai-collect_v2/2.0.0 (by /u/ai_researcher)'
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Run Full Crawl
        if: ${{ github.event.inputs.crawl_type == 'full' }}
        run: |
          echo "ğŸš€ å¼€å§‹å®Œæ•´é‡‡é›† - ç›®æ ‡ ${{ github.event.inputs.target_count || '2000' }} æ¡å¸–å­..."
          node scripts/full-crawl-2000.js
          echo "ğŸ“¥ å¼€å§‹æ•°æ®åº“æ’å…¥..."
          node scripts/direct-d1-insert.js
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: 'reddit-ai-collect_v2/2.0.0 (by /u/ai_researcher)'
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

      - name: Upload Crawl Data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: crawl-data-${{ github.run_number }}-${{ github.event.inputs.crawl_type || 'incremental' }}
          path: data/
          retention-days: 7

      - name: Database Statistics
        if: success()
        run: |
          echo "ğŸ“Š è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯..."
          node -e "
          const CLOUDFLARE_API_TOKEN = process.env.CLOUDFLARE_API_TOKEN;
          const ACCOUNT_ID = process.env.ACCOUNT_ID;
          const DATABASE_ID = process.env.DATABASE_ID;
          
          async function getStats() {
            const response = await fetch(\`https://api.cloudflare.com/client/v4/accounts/\${ACCOUNT_ID}/d1/database/\${DATABASE_ID}/query\`, {
              method: 'POST',
              headers: {
                'Authorization': \`Bearer \${CLOUDFLARE_API_TOKEN}\`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({ 
                sql: 'SELECT COUNT(*) as total, COUNT(DISTINCT subreddit) as subreddits, COUNT(DISTINCT collection_date) as days FROM redditV2_posts;' 
              })
            });
            
            const result = await response.json();
            const stats = result.result[0]?.results?.[0];
            
            console.log('ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:');
            console.log(\`ğŸ“„ æ€»å¸–å­æ•°: \${stats.total}\`);
            console.log(\`ğŸ  è¦†ç›–ç¤¾åŒº: \${stats.subreddits}\`);
            console.log(\`ğŸ“… é‡‡é›†å¤©æ•°: \${stats.days}\`);
            
            // è®¾ç½®GitHub Actionsè¾“å‡º
            console.log(\`::notice title=é‡‡é›†å®Œæˆ::æ€»è®¡\${stats.total}æ¡å¸–å­ï¼Œè¦†ç›–\${stats.subreddits}ä¸ªç¤¾åŒº\`);
          }
          
          getStats().catch(console.error);
          "
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
          DATABASE_ID: ${{ secrets.DATABASE_ID }}

  notify:
    runs-on: ubuntu-latest
    name: é€šçŸ¥çŠ¶æ€
    needs: crawl
    if: always()
    steps:
      - name: Notify Success
        if: needs.crawl.result == 'success'
        run: |
          echo "âœ… Reddit AIæ•°æ®é‡‡é›†ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ!"
          echo "ğŸ¯ é‡‡é›†ç±»å‹: ${{ github.event.inputs.crawl_type || 'incremental' }}"
          echo "ğŸ“… æ‰§è¡Œæ—¶é—´: $(date)"
          
      - name: Notify Failure
        if: needs.crawl.result == 'failure'
        run: |
          echo "âŒ Reddit AIæ•°æ®é‡‡é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥!"
          echo "ğŸ¯ é‡‡é›†ç±»å‹: ${{ github.event.inputs.crawl_type || 'incremental' }}"
          echo "ğŸ“… æ‰§è¡Œæ—¶é—´: $(date)"
          echo "::error title=é‡‡é›†å¤±è´¥::è¯·æ£€æŸ¥æ—¥å¿—å¹¶é‡æ–°è¿è¡Œ"
